\documentclass{article}
\usepackage{layout-notes}
\usepackage{statmacros}
\bibliography{biblio}
\author{Federica Stolf$ ^{1}$, Gian Luca Vriz$ ^{1}$, Daniele Zago$^1$}
\title{Title}
\date{%
\smaller
$^1$ \textit{Department of Statistics, University of Padua, Padua, Italy}\\%
\today}
\begin{document}
\maketitle

\section{Objective}

Off-policy evaluation (OPE) aims to estimate the performance of hypothetical policies using data generated by a different policy. Nowadays, there has been growing research interest in this field. 

Interactive bandit and reinforcement learning systems (e.g., personalized medicine, recommendation, etc.) produce log data which can be used for evaluating/improving the original system.

For instance, the logs of a news recommendation system record which news article was presented and whether the user read it, giving the system designer a chance to make an improve in his/her private revenue.
The problem is that, exploiting log bandit data is sometimes difficult. Moreover, the logs are biased due to they over-represent the actions favored by the system. The more natural solution to this problem is an \textit{A/B} test (which compares the performance of different systems in an online environment). However, \textit{A/B} testing is often difficult to perform. Deploying a new policy requires high levels of both time and money consuming. This increases exponentially the risk of failure for the policymaker; opening the way to the problem of off-policy evaluation. Its aim is to estimate the target policy using only log data collected by the past. The main advantage is that OPE allows us to compare the performance of a policy without implementing \textit{A/B} tests or in other words, it is able to safe entrepreneurial revenues minimising the risk of failure. 

\textbf{The aim:} Under this setting, our goal is to implement and evaluate a reasonable algorithm which is able to minimize the expected loses of the system designer. To estimate the counterfactual performance of hypothetical algorithm we will rely on OPE techniques.
\section{The data}

We use Open Bandit Dataset (https://research.zozo.com/data.html). The dataset is provided by ZOZO, Inc., the largest Japanese fashion e-commerce company 

It is constructed in an A/B test of two multi-armed bandit policies in a large-scale fashion e-commerce platform, ZOZOTOWN. It currently consists of a total of 26M rows, each one representing a user impression with some feature values, selected items as actions, true propensity scores, and click indicators as an outcome. 

The data have the following variables:
\begin{itemize}
    \item \texttt{item id}: index of items as arms (index ranges from 0-33 for "Men" campaign).
    \item \texttt{position}: the position of an item being recommended (1, 2, or 3 correspond to left, center, and right position of the ZOZOTOWN recommendation interface, respectively).
    \item \texttt{click}: target variable that indicates if an item was clicked (1) or not (0).
    \item \texttt{propensity score}: the probability of an item being recommended at each position.
    \item  \texttt{user feature}: 0-4 user-related feature values.
\end{itemize}



\section{Methods}
Given the struture of the dataset, our first goal is to compare standard multi-armed bandit strategies in order to find the most promising item recommendations.
Our analysis will begin by applying bandit algorithms that do not leverage covariate information, such as standard Thompson sampling and UCB algorithms.
Each algorithm will be compared with the ground truth given by the best policy calculated over the whole dataset over multiple runs in order to obtain a distribution of the performance. \\

As a first extension, a regression-based algorithm (contextual bandit policy) may leverage the available user covariates to improve the performance. 
\printbibliography
\end{document}

