\documentclass{article}
\usepackage{layout-notes}
\usepackage{statmacros}
\bibliography{biblio}
\author{Farooq Ahmad$^1$, Federica Stolf$ ^{1}$, Gian Luca Vriz$ ^{1}$, Daniele Zago$^1$}
\title{Project proposal}
\date{%
\smaller
$^1$ \textit{Department of Statistics, University of Padua, Padua, Italy}\\%
\today}
\begin{document}
\maketitle

\section{Objective}

Off-policy evaluation (OPE) aims to estimate the performance of hypothetical policies using data generated by a different policy.
For instance, the logs of a news recommendation system record which news article was presented and whether the user read it, giving the system designer a chance to make an improve in his/her private revenue.
Nowadays, there has been growing research interest in using OPE due to the fact that multi-armed bandit algorithms are usually tested over simulated data, which is usually unrealistic when compared to the actual data that is encountered in real applications.
Instead, by relying on data collected during an extended period of time it is possible to compare the bandit algorithms and see whether an improvement can be obtained for the population of interest, i.e. the user base.

\paragraph{The aim:} Our goal is to implement and evaluate a reasonable algorithm which is able to minimize the expected loses of the system designer.
To estimate the performance of hypothetical algorithm we will rely on a dataset which has been collected during actual user-item interactions, which provides a realistic framework to test the proposed methods.
\section{The data}

We consider the Open Bandit Dataset (https://research.zozo.com/data.html). The dataset is provided by ZOZO, Inc., the largest Japanese fashion e-commerce company. The platform use Bernoulli Thompson Sampling (Bernoulli TS) and uniform random (Random) policies to recommend fashion items to users. 
The dataset thus includes a set of
two logged user-item interaction datasets. 

Considering the campaign for male users (\texttt{Men}), the data currently consists of a total of 452949 rows, each one containing the following observed variables:
% a user impression with some feature values, selected items as actions, true propensity scores, and click indicators as an outcome. 
\begin{itemize}
    \item \texttt{item id}: index of proposed items as arms (index ranges from 0-33 for the \texttt{Men} campaign).
    \item \texttt{click}: target variable that indicates if an item was clicked (1) or not (0).
    \item \texttt{position}: the position of an item being recommended (1, 2, or 3 correspond to left, center, and right position of the ZOZOTOWN recommendation interface, respectively).
    \item  \texttt{user feature 0-4}: user-related feature values.
    \item \texttt{propensity score}: the probability of an item being recommended at each position.
\end{itemize}



\section{Methods}
Given the struture of the datasets, our first goal is to compare standard multi-armed bandit strategies with the random policy in order to find the most promising item recommendations.
\begin{enumerate}
    \item We use the \texttt{random} dataset, which has been collected by randomly proposing the items, as a proxy of the population and then apply the multi-armed bandit methods multiple times to obtain a distribution of the algorithm performance.
    Perhaps we can also incorporate multiple bootstrap replicates from the dataset to also include variability due to the sampling from the target population.
    
    \item  The analysis will first compare a completely random strategy with bandit algorithms that do not leverage covariate information, such as standard Thompson sampling and UCB algorithms. We will compare different algorithm to see which one maximize the cumulative distribution function of click rate.
    
    \item We will perform a simulation study and compare the results with the ones obtained with the real data, in order to evaluate any differences and outline possible suggestions about how to improve the simulation framework.
    
    
    \item Finally, as a possible extension, a regression-based algorithm (contextual bandit methods) may leverage the available user or item covariates in order to improve the algorithm performance. 
\end{enumerate}

\section{Discussion}
There might be some problems due to the high imbalance between 0's and 1's in the response variable (\texttt{click}).
This is a standard occurrence in real datasets concerning click rates on websites, and may prove itself to be problematic when applying the bootstrap resampling procedure.




\printbibliography
\end{document}

